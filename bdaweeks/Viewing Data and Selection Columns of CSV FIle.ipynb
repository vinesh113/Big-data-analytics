{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5e3474e-06e9-472e-9b36-a2b29bb15859",
   "metadata": {},
   "source": [
    "Create a DataFrame in PySpark and apply basic operations such as viewing data and selecting columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11a1b1b4-a76d-4475-949a-35e1cf3b31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "\n",
    "# Point to Java & Spark\n",
    "os.environ[\"JAVA_HOME\"] = \"C:/Progra~1/Java/jdk1.8\"\n",
    "os.environ[\"SPARK_HOME\"] = \"C:/spark/spark-3.5.7-bin-hadoop3-scala2.13\"\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:/hadoop\"   # if you installed winutils here\n",
    "os.environ[\"PATH\"] += \";C:/spark/spark-3.5.7-bin-hadoop3-scala2.13/bin;C:/hadoop/bin\"\n",
    "\n",
    "# Initialize findspark\n",
    "findspark.init(os.environ[\"SPARK_HOME\"])\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Now build SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV_RDD_Example\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7cc5785-348f-4393-a76a-dd193cc5cf72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://checkhost.local:4043\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>CSV_RDD_Example</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=CSV_RDD_Example>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d612e8c9-fe16-4381-a968-68ac21fb6867",
   "metadata": {},
   "source": [
    "ðŸ“Š Dataset Overview\n",
    "\n",
    "    --Total Records: 50 students\n",
    "    \n",
    "    --Columns: 7 â†’ id, name, age, gender, math, science, english\n",
    "    \n",
    "    --No missing values\n",
    "    \n",
    "ðŸ‘¥ Demographics  \n",
    "\n",
    "    --Age: 18 â€“ 25 years (average â‰ˆ 21.5)\n",
    "    \n",
    "    --Gender: 29 Female, 21 Male\n",
    "    \n",
    "ðŸ“š Academic Performance\n",
    "\n",
    "**Math:\n",
    "\n",
    "    --Range: 40 â€“ 100\n",
    "    \n",
    "    --Mean: 68.9\n",
    "    \n",
    "    --Std. Dev.: 17.6 (high variation)\n",
    "    \n",
    "**Science:\n",
    "\n",
    "    --Range: 44 â€“ 99\n",
    "    \n",
    "    --Mean: 70.2\n",
    "    \n",
    "    --Std. Dev.: 14.6 (moderate variation)\n",
    "    \n",
    "**English:\n",
    "\n",
    "    --Range: 42 â€“ 100\n",
    "    \n",
    "    --Mean: 69.4\n",
    "    \n",
    "    --Std. Dev.: 18.7 (highest variation)\n",
    "\n",
    "âœ¨ Key Insights  \n",
    "\n",
    "    --Science is the strongest subject on average.\n",
    "    \n",
    "    --English has the most variation in performance.\n",
    "    \n",
    "    --Students perform differently across subjects (not uniform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e83a89-ef53-4fc3-969c-e0ec6f92c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    " from pyspark.sql import SparkSession\n",
    " # Step 1: Initialize Spark Session\n",
    " spark = SparkSession.builder.appName(\"SamplingExample\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87b380e7-c4b8-46e2-9ec4-fc859c84e5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 2: Read CSV file into DataFrame\n",
    " df = spark.read.csv(\"students.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a25d7c01-e30e-42bf-a5fc-570bbefc76ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== First 5 rows of dataset ===\n",
      "+---+-------+---+------+----+-------+-------+\n",
      "| id|   name|age|gender|math|science|english|\n",
      "+---+-------+---+------+----+-------+-------+\n",
      "|  1|  Alice| 20|     F|  66|     92|     44|\n",
      "|  2|    Bob| 20|     M|  82|     52|     77|\n",
      "|  3|Charlie| 22|     F|  43|     57|     76|\n",
      "|  4|  David| 19|     M|  95|     69|     46|\n",
      "|  5|    Eva| 19|     F|  62|     44|     96|\n",
      "+---+-------+---+------+----+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # === Sampling Demonstration (within 7 operations) ===\n",
    " # 1. View first 5 rows\n",
    " print(\"=== First 5 rows of dataset ===\")\n",
    " df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3472a523-541f-4917-a333-6fb3e76e35ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Schema of dataset ===\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- math: integer (nullable = true)\n",
      " |-- science: integer (nullable = true)\n",
      " |-- english: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # 2. Print schema\n",
    " print(\"=== Schema of dataset ===\")\n",
    " df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75fb13e7-22e5-413c-9d14-8448f0980550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample (30% without replacement) ===\n",
      "+---+------+---+------+----+-------+-------+\n",
      "| id|  name|age|gender|math|science|english|\n",
      "+---+------+---+------+----+-------+-------+\n",
      "|  4| David| 19|     M|  95|     69|     46|\n",
      "|  8| Henry| 21|     F|  53|     82|     60|\n",
      "| 17|Quincy| 18|     M|  65|     79|     54|\n",
      "| 19|   Sam| 18|     F|  76|     70|     65|\n",
      "| 27| Aaron| 25|     F|  81|     99|     44|\n",
      "| 28| Bella| 19|     F|  54|     76|     76|\n",
      "| 32| Fiona| 22|     F|  48|     96|     48|\n",
      "| 37|  Kyle| 21|     M|  57|     86|     92|\n",
      "| 39|  Matt| 25|     M|  64|     71|    100|\n",
      "| 41| Oscar| 20|     M|  87|     72|     81|\n",
      "+---+------+---+------+----+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # 3. Random sample without replacement (30% of data)\n",
    " print(\"=== Sample (30% without replacement) ===\")\n",
    " df.sample(withReplacement=False, fraction=0.3, seed=42).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba8f5b4-8ee8-41e3-b1dc-2bcc6d3749c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Sample (20% with replacement) ===\n",
      "+---+------+---+------+----+-------+-------+\n",
      "| id|  name|age|gender|math|science|english|\n",
      "+---+------+---+------+----+-------+-------+\n",
      "|  6| Frank| 22|     F|  70|     78|     94|\n",
      "|  7| Grace| 24|     F|  67|     66|     93|\n",
      "| 14|Nathan| 23|     F|  71|     66|     60|\n",
      "| 17|Quincy| 18|     M|  65|     79|     54|\n",
      "| 21|   Uma| 19|     F|  89|     70|     76|\n",
      "| 22|Victor| 22|     M|  96|     75|     56|\n",
      "| 31| Ethan| 24|     M|  53|     57|     45|\n",
      "| 32| Fiona| 22|     F|  48|     96|     48|\n",
      "| 35|   Ian| 21|     F|  72|     75|     70|\n",
      "| 38| Laura| 23|     M|  84|     73|     56|\n",
      "+---+------+---+------+----+-------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # 4. Random sample with replacement (20% of data)\n",
    " print(\"=== Sample (20% with replacement) ===\")\n",
    " df.sample(withReplacement=True, fraction=0.2, seed=42).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c2827e5-897f-4d8c-8753-6ba8fa1a185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== takeSample: 5 rows (without replacement) ===\n",
      "Row(id=35, name='Ian', age=21, gender='F', math=72, science=75, english=70)\n",
      "Row(id=26, name='Zoey', age=18, gender='M', math=42, science=48, english=42)\n",
      "Row(id=17, name='Quincy', age=18, gender='M', math=65, science=79, english=54)\n",
      "Row(id=43, name='Quinn', age=18, gender='F', math=56, science=60, english=87)\n",
      "Row(id=38, name='Laura', age=23, gender='M', math=84, science=73, english=56)\n"
     ]
    }
   ],
   "source": [
    "# 5. Take a random sample of 5 rows using takeSample (without replacement)\n",
    "print(\"=== takeSample: 5 rows (without replacement) ===\")\n",
    "sampled_rows = df.rdd.takeSample(False, 5, seed=42)\n",
    "for row in sampled_rows:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4344af53-2c01-42a9-b176-057302d098be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== takeSample: 5 rows (with replacement) ===\n",
      "Row(id=47, name='Umar', age=21, gender='F', math=75, science=80, english=59)\n",
      "Row(id=17, name='Quincy', age=18, gender='M', math=65, science=79, english=54)\n",
      "Row(id=10, name='Jack', age=19, gender='F', math=44, science=59, english=60)\n",
      "Row(id=38, name='Laura', age=23, gender='M', math=84, science=73, english=56)\n",
      "Row(id=23, name='Wendy', age=24, gender='M', math=57, science=83, english=81)\n"
     ]
    }
   ],
   "source": [
    " # 6. Take a random sample of 5 rows using takeSample (with replacement)\n",
    " print(\"=== takeSample: 5 rows (with replacement) ===\")\n",
    " sampled_rows_wr = df.rdd.takeSample(True, 5, seed=42)\n",
    " for row in sampled_rows_wr:\n",
    "     print(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "933fc892-05a4-4359-9662-67e35f7761f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in dataset: 50\n"
     ]
    }
   ],
   "source": [
    " # 7. Count total rows (to compare with sampled data size)\n",
    " print(\"Total rows in dataset:\", df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101eff4-2a46-434c-9913-ef9428e92ff8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0739c8e-f5d6-4c5a-b0f2-334c5ce25583",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "    --Initialization: The notebook initializes a Spark session and reads a CSV file named students.csv into a DataFrame.\n",
    "\n",
    "    --Data Viewing: It shows the first 5 rows of the DataFrame.\n",
    "\n",
    "    --Schema and Columns: It prints the schema of the DataFrame, showing the column names and data types, and also lists all column names.\n",
    "\n",
    "    --Column Selection: It selects and displays a subset of the data, specifically the name and math columns.\n",
    "\n",
    "    --Filtering: It filters the DataFrame to show only students with a math score greater than or equal to 80.\n",
    "\n",
    "    --Sorting: It sorts the students in descending order based on their science marks and displays the top 5 results.\n",
    "\n",
    "    --Counting: It counts and prints the total number of rows in the dataset, which is 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "689fb03f-36b5-4a38-807c-8e4d765cf49b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Stop Spark session\n",
    " # spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05f0e7f-60f8-4f62-95b9-aa376ab47e52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (py310)",
   "language": "python",
   "name": "pyspark310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
