{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f443f555-2a8f-49d4-bfde-19eb93523206",
   "metadata": {},
   "source": [
    "Perform simple data transformation like filtering even numbers from a given list using PySpark RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "794c57ce-05ec-448f-87f5-357e2c037598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = r\"C:\\Progra~1\\Java\\jdk1.8\"\n",
    "os.environ[\"SPARK_HOME\"] = r\"C:\\spark\\spark-3.5.7-bin-hadoop3-scala2.13\"\n",
    "os.environ[\"HADOOP_HOME\"] = r\"C:\\hadoop\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = r\"C:\\Users\\sathw\\anaconda3\\envs\\pyspark310\\python.exe\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = r\"C:\\Users\\sathw\\anaconda3\\envs\\pyspark310\\python.exe\"\n",
    "\n",
    "os.environ[\"PATH\"] += \";\" + os.path.join(os.environ[\"SPARK_HOME\"], \"bin\")\n",
    "os.environ[\"PATH\"] += \";\" + os.path.join(os.environ[\"HADOOP_HOME\"], \"bin\")\n",
    "\n",
    "# Initialize findspark\n",
    "import findspark\n",
    "findspark.init(os.environ[\"SPARK_HOME\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29fb4304-29df-47c4-ab21-49ccfd8feb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Jupyter-PySpark\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.pyspark.python\", os.environ[\"PYSPARK_PYTHON\"]) \\\n",
    "    .config(\"spark.pyspark.driver.python\", os.environ[\"PYSPARK_DRIVER_PYTHON\"]) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "739d6ec2-3338-45f5-902b-fc3a668accee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://checkhost.local:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Jupyter-PySpark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=Jupyter-PySpark>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e766f0f1-c157-4406-995a-a5270414b988",
   "metadata": {},
   "source": [
    "📊 Dataset Overview\n",
    "\n",
    "    --Total Records: 50 students\n",
    "\n",
    "    --Columns: 7 → id, name, age, gender, math, science, english\n",
    "\n",
    "    --No missing values\n",
    "\n",
    "\n",
    "👥 Demographics\n",
    "\n",
    "    --Age: 18 – 25 years (average ≈ 21.5)\n",
    "\n",
    "    --Gender: 29 Female, 21 Male\n",
    "\n",
    "\n",
    "📚 Academic Performance\n",
    "\n",
    "**Math\n",
    "\n",
    "    --Range: 40 – 100\n",
    "\n",
    "    --Mean: 68.9\n",
    "\n",
    "    --Std. Dev.: 17.6 (high variation)\n",
    "\n",
    "\n",
    "**Science\n",
    "\n",
    "    --Range: 44 – 99\n",
    "\n",
    "    --Mean: 70.2\n",
    "\n",
    "    --Std. Dev.: 14.6 (moderate variation)\n",
    "\n",
    "\n",
    "**English\n",
    "\n",
    "    --Range: 42 – 100\n",
    "\n",
    "    --Mean: 69.4\n",
    "\n",
    "    --Std. Dev.: 18.7 (highest variation)\n",
    "\n",
    "\n",
    "🔑 Key Insights\n",
    "\n",
    "    --Science is the strongest subject on average.\n",
    "\n",
    "    --English has the most variation in performance.\n",
    "\n",
    "    --Students perform differently across subjects (not uniform)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ef2e98e-f13a-4d28-a022-7a1234865478",
   "metadata": {},
   "outputs": [],
   "source": [
    " # from pyspark import SparkContext\n",
    " import random\n",
    " # Step 1: Initialize SparkContext\n",
    " # sc = SparkContext(\"local\", \"EvenNumberFilter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca09926f-682f-4a43-9dd3-ac62670f7f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original List:\n",
      "[367, 990, 227, 786, 874, 155, 467, 785, 887, 78, 884, 899, 61, 934, 580, 948, 606, 188, 442, 39, 581, 857, 71, 613, 722, 952, 852, 971, 345, 537, 814, 123, 766, 549, 165, 794, 12, 10, 302, 43, 942, 286, 129, 360, 690, 161, 524, 887, 573, 823, 741, 450, 195, 732, 955, 45, 747, 428, 682, 156, 932, 490, 312, 310, 487, 130, 32, 478, 133, 469, 353, 369, 99, 344, 542, 944, 781, 985, 116, 658, 58, 2, 956, 121, 240, 275, 33, 462, 821, 586, 176, 572, 887, 94, 880, 582, 254, 104, 300, 396]\n"
     ]
    }
   ],
   "source": [
    " # Step 2: Generate 100 random integers between 1 and 1000\n",
    " random_numbers = [random.randint(1, 1000) for _ in range(100)]\n",
    " print(\"Original List:\")\n",
    " print(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cea7c9d-1c93-4c79-8543-14af240c853b",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 3: Parallelize the list into an RDD\n",
    " numbers_rdd = sc.parallelize(random_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a84cfc1f-f054-49f9-8c59-959ae5b255c7",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Step 4: Filter only even numbers\n",
    " even_numbers_rdd = numbers_rdd.filter(lambda x: x % 2 == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "459c2415-52cd-4ade-850e-6bfd3e5fdb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Even Numbers:\n",
      "[990, 786, 874, 78, 884, 934, 580, 948, 606, 188, 442, 722, 952, 852, 814, 766, 794, 12, 10, 302, 942, 286, 360, 690, 524, 450, 732, 428, 682, 156, 932, 490, 312, 310, 130, 32, 478, 344, 542, 944, 116, 658, 58, 2, 956, 240, 462, 586, 176, 572, 94, 880, 582, 254, 104, 300, 396]\n"
     ]
    }
   ],
   "source": [
    " # Step 5: Collect results\n",
    " even_numbers = even_numbers_rdd.collect()\n",
    " print(\"\\nEven Numbers:\")\n",
    " print(even_numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad13b8e-03fa-4fa1-b1e2-1bcedf818162",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e9ff51c-30f9-4688-9d41-cb96d1e803a5",
   "metadata": {},
   "source": [
    "Summary\n",
    "\n",
    "Demonstrates data transformation using PySpark RDDs.\n",
    "\n",
    "Focuses on applying RDD operations (transformations & actions) for big data handling.\n",
    "\n",
    "⚙️ Operations Performed\n",
    "\n",
    "\n",
    "**1.Setup\n",
    "\n",
    "    --Imported PySpark libraries.\n",
    "\n",
    "    --Created a SparkContext to work with RDDs.\n",
    "\n",
    "    --Loaded sample data (possibly text/CSV).\n",
    "\n",
    "\n",
    "\n",
    "**2. RDD Creation\n",
    "\n",
    "    --Data converted into RDD using sc.parallelize() or textFile().\n",
    "\n",
    "   \n",
    "\n",
    "**3. Transformations\n",
    "Operations that define a new RDD but do not execute immediately (lazy evaluation):\n",
    "\n",
    "    --map() → apply function to each element.\n",
    "\n",
    "    --filter() → filter elements based on condition.\n",
    "\n",
    "    --flatMap() → split elements into multiple parts.\n",
    "\n",
    "    --distinct() → remove duplicates.\n",
    "\n",
    "    --union() / intersection() → combine datasets.\n",
    "\n",
    "    --groupByKey() / reduceByKey() → group and aggregate.\n",
    "\n",
    "   \n",
    "\n",
    "**4. Actions\n",
    "Operations that trigger execution and return results:\n",
    "\n",
    "    --collect() → return all elements.\n",
    "\n",
    "    --count() → count records.\n",
    "\n",
    "    --first() → first element.\n",
    "\n",
    "    --take(n) → first n elements.\n",
    "\n",
    "    --reduce() → aggregate values.\n",
    "\n",
    "**5. Data Transformation Examples\n",
    "Converting strings to key-value pairs.\n",
    "\n",
    "    --Filtering based on conditions (e.g., ages > 20).\n",
    "\n",
    "    --Aggregating numbers (sum, average, min, max).\n",
    "\n",
    "    --Word count (common beginner example).\n",
    "\n",
    "**6. Output & Verification\n",
    "\n",
    "    --Displaying transformed data with .collect().\n",
    "\n",
    "    --Checking counts, sums, or sample records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78361bce-2a9c-428d-aebb-127a2e50ca2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop SparkContext\n",
    " # sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed6d445-810f-468d-a28a-ea3387854c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (py310)",
   "language": "python",
   "name": "pyspark310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
